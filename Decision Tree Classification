
\\"Email Spam Detection-DecTreeclass"
\\Decision Tree Classification:
Decision tree classification is same as decision tree regression however it uses rules and brances to assign a classification group (such as 1 or 0) instead of an exact value. This dataset is used for spam detection from Kaggle. It contains attributes that indicate whether a particular word or character was frequently occurring in a e-mail and uses that to determine spamm. It can also be personalized by extrapolating data from your email database and used as personal spam detector! 
Importing the libraries
These are the three go to libraries for most ML.

In [23]:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
 Importing the dataset
Simple dataset import using Pandas dataframe and iloc to assign our independent variable(s) (everything besides the last column) and our dependent variable (the last column). The name of the dataset has to be updated and it must be in the same folder as your .py file or uploaded on Jupyter Notebooks or Google Collab.

In [24]:
dataset = pd.read_csv('spambase_csv.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
Splitting the dataset into the Training set and Test set
I used a 80/20 split. The random state is tuned to 0 for consistency sakes.

In [25]:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)
Feature Scaling
In [26]:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
Training the Decision Tree model on the whole dataset
We use the fit method to train our DecisionTreeRegressor on the entire dataset.

In [27]:
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)
Out[27]:
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=0, splitter='best')

Predicting the Test set results:
By using the concatenate function I display the predicted values and actual values in a side by side 2D array through '(len(y_wtv), 1))' for easy viewing.

In [28]:
y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
[[1 1]
 [0 0]
 [0 0]
 ...
 [0 0]
 [0 0]
 [1 1]]
Confusion Matrix:
The confusion matrix is a useful metric for classification models to allow us to visualize the correct positive, false positive, false negative, and correct negative predictions as well as a ultimate accuracy score on the bottom. Our model reached a accuracy of 91%!

In [29]:
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)
[[500  38]
 [ 42 341]]
Out[29]:
0.9131378935939196

Program for implementation:

# -*- coding: utf-8 -*-
"""decision_tree_classification.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1iEm9YnBKmk__sAq09QdUOqoWEEK_6MA8
# Decision Tree Classification
Decision tree classification is same as decision tree regression however it uses rules and brances to assign a classification group (such as 1 or 0) instead of an exact value. This dataset is used for spam detection from Kaggle. It contains attributes that indicate whether a particular word or character was frequently occurring in a e-mail and uses that to determine spamm. It can also be personalized by extrapolating data from your email database and used as personal spam detector! Here is the [link](https://www.kaggle.com/somesh24/spambase).
### Importing the libraries
These are the three go to libraries for most ML.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""### Importing the dataset
Simple dataset import using Pandas dataframe and iloc to assign our independent variable(s) (everything besides the last column) and our dependent variable (the last column). The name of the dataset has to be updated and it must be in the same folder as your .py file or uploaded on Jupyter Notebooks or Google Collab.
"""

dataset = pd.read_csv('spambase_csv.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""### Splitting the dataset into the Training set and Test set
I used a 80/20 split. The random state is tuned to 0 for consistency sakes.
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""### Training the Decision Tree  model on the whole dataset
We use the fit method to train our DecisionTreeRegressor on the entire dataset.
"""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

"""### Predicting the Test set results
By using the concatenate function I display the predicted values and  actual values in a side by side 2D array through '(len(y_wtv), 1))' for easy viewing.
"""

y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""## Confusion Matrix
The confusion matrix is a useful metric for classification models to allow us to visualize the correct positive, false positive, false negative, and correct negative predictions as well as a ultimate accuracy score on the bottom. Our model reached a accuracy of 91%!
"""

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)
